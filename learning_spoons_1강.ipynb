{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "learning spoons 1강.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ohnownihs/NLP/blob/master/learning_spoons_1%EA%B0%95.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7hJxnNnALVmR",
        "colab_type": "text"
      },
      "source": [
        "# Tokenization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iciE21O6LaLg",
        "colab_type": "text"
      },
      "source": [
        "## Word Tokenization(영어)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GhyKqRIS-BeP",
        "colab_type": "text"
      },
      "source": [
        "단어 단위로 토큰화를 수행하는 것을 Word Tokenization이라고 합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h7dq1AIa4YQw",
        "colab_type": "text"
      },
      "source": [
        "### **NLTK의 토크나이저 비교**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u9vsOXPY3Wqe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ofn4FR4V3Ynv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "2d29e019-973a-4015-ba5f-ef4ef2854d32"
      },
      "source": [
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q_v2UoFX3DOr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentence = \"Don't be fooled by the dark sounding name, Mr. Jone's Orphanage is as cheery as cheery goes for a pastry shop.\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tg_eQnb73i9Z",
        "colab_type": "text"
      },
      "source": [
        "**아포스트로피가 들어간 상황에서 Don't와 Jone's는 어떻게 토큰화할 수 있을까요?**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZG-ES853DRs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "cb18896b-0074-4fe7-8361-e861156a7fc1"
      },
      "source": [
        "from nltk.tokenize import word_tokenize  \n",
        "print(word_tokenize(sentence))  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Do', \"n't\", 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', ',', 'Mr.', 'Jone', \"'s\", 'Orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWnG6YxT3qp_",
        "colab_type": "text"
      },
      "source": [
        "Don't를 Do와 n't로 분리하였으며, Jone's는 Jone과 's로 분리"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tbwe2_ny3DWj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "5f84dec5-ca7b-47a5-a639-2b4641dfdf39"
      },
      "source": [
        "from nltk.tokenize import WordPunctTokenizer  \n",
        "print(WordPunctTokenizer().tokenize(sentence))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Don', \"'\", 't', 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', ',', 'Mr', '.', 'Jone', \"'\", 's', 'Orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ErJrv2nT3sMP",
        "colab_type": "text"
      },
      "source": [
        "Don't를 Don과 '와 t로 분리하였으며, Jone's를 Jone과 '와 s로 분리"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YFuuDjG_3_i-",
        "colab_type": "text"
      },
      "source": [
        "**정답은 없습니다. 사실 토크나이저마다 각자 규칙이 다르기 때문에 사용하고자 하는 목적에 따라 토크나이저를 선택하는 것이 중요합니다.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zot_67ckA4TZ",
        "colab_type": "text"
      },
      "source": [
        "Penn Treebank Tokenization 규칙  \n",
        "\n",
        "규칙 1. 하이푼으로 구성된 단어는 하나로 유지한다.  \n",
        "규칙 2. doesn't와 같이 아포스트로피로 '접어'가 함께하는 단어는 분리해준다.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sTDxdxLk4-IG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "ae620a48-e417-4809-e2e3-39553df862a9"
      },
      "source": [
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "tokenizer=TreebankWordTokenizer()\n",
        "text = \"Starting a home-based restaurant may be an ideal. it doesn't have a food chain or restaurant of their own.\"\n",
        "print(tokenizer.tokenize(text))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Starting', 'a', 'home-based', 'restaurant', 'may', 'be', 'an', 'ideal.', 'it', 'does', \"n't\", 'have', 'a', 'food', 'chain', 'or', 'restaurant', 'of', 'their', 'own', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uSMrszdyBLQj",
        "colab_type": "text"
      },
      "source": [
        "## Word Tokenization(한국어)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QXKm73rpBRg8",
        "colab_type": "text"
      },
      "source": [
        "한국어의 특성으로 인해 영어와는 토큰화가 좀 더 까다롭습니다.  \n",
        "영어의 경우에는 사실 띄어쓰기로 토큰화를 하더라도 큰 문제가 되지 않는 경우가 많은데  \n",
        "한국어의 경우 어절(띄어쓰기 단위) 토큰화는 지양합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pQMm9UPdBhCP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 600
        },
        "outputId": "a4274195-f161-4fba-fe30-b396d5dd30f6"
      },
      "source": [
        "!pip install konlpy"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting konlpy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/85/0e/f385566fec837c0b83f216b2da65db9997b35dd675e107752005b7d392b1/konlpy-0.5.2-py2.py3-none-any.whl (19.4MB)\n",
            "\u001b[K     |████████████████████████████████| 19.4MB 14.0MB/s \n",
            "\u001b[?25hCollecting beautifulsoup4==4.6.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/d4/10f46e5cfac773e22707237bfcd51bbffeaf0a576b0a847ec7ab15bd7ace/beautifulsoup4-4.6.0-py3-none-any.whl (86kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 11.3MB/s \n",
            "\u001b[?25hCollecting JPype1>=0.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2d/9b/e115101a833605b3c0e6f3a2bc1f285c95aaa1d93ab808314ca1bde63eed/JPype1-0.7.5-cp36-cp36m-manylinux2010_x86_64.whl (3.6MB)\n",
            "\u001b[K     |████████████████████████████████| 3.6MB 39.0MB/s \n",
            "\u001b[?25hCollecting tweepy>=3.7.0\n",
            "  Downloading https://files.pythonhosted.org/packages/36/1b/2bd38043d22ade352fc3d3902cf30ce0e2f4bf285be3b304a2782a767aec/tweepy-3.8.0-py2.py3-none-any.whl\n",
            "Collecting colorama\n",
            "  Downloading https://files.pythonhosted.org/packages/c9/dc/45cdef1b4d119eb96316b3117e6d5708a08029992b2fee2c143c7a0a5cc5/colorama-0.4.3-py2.py3-none-any.whl\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.6/dist-packages (from konlpy) (4.2.6)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.6/dist-packages (from konlpy) (1.18.5)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (1.12.0)\n",
            "Requirement already satisfied: requests>=2.11.1 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (2.23.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (1.3.0)\n",
            "Requirement already satisfied: PySocks>=1.5.7 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (1.7.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.11.1->tweepy>=3.7.0->konlpy) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.11.1->tweepy>=3.7.0->konlpy) (2.9)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.11.1->tweepy>=3.7.0->konlpy) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.11.1->tweepy>=3.7.0->konlpy) (2020.4.5.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (3.1.0)\n",
            "Installing collected packages: beautifulsoup4, JPype1, tweepy, colorama, konlpy\n",
            "  Found existing installation: beautifulsoup4 4.6.3\n",
            "    Uninstalling beautifulsoup4-4.6.3:\n",
            "      Successfully uninstalled beautifulsoup4-4.6.3\n",
            "  Found existing installation: tweepy 3.6.0\n",
            "    Uninstalling tweepy-3.6.0:\n",
            "      Successfully uninstalled tweepy-3.6.0\n",
            "Successfully installed JPype1-0.7.5 beautifulsoup4-4.6.0 colorama-0.4.3 konlpy-0.5.2 tweepy-3.8.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2j4Q8HvpGrgd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "49b77033-12f7-4523-bf5f-7e713571cdb8"
      },
      "source": [
        "# Colab에 Mecab 설치\n",
        "!git clone https://github.com/SOMJANG/Mecab-ko-for-Google-Colab.git\n",
        "%cd Mecab-ko-for-Google-Colab\n",
        "!bash install_mecab-ko_on_colab190912.sh"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'Mecab-ko-for-Google-Colab'...\n",
            "remote: Enumerating objects: 60, done.\u001b[K\n",
            "remote: Counting objects: 100% (60/60), done.\u001b[K\n",
            "remote: Compressing objects: 100% (55/55), done.\u001b[K\n",
            "remote: Total 60 (delta 23), reused 20 (delta 5), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (60/60), done.\n",
            "/content/Mecab-ko-for-Google-Colab\n",
            "Installing konlpy.....\n",
            "Requirement already satisfied: konlpy in /usr/local/lib/python3.6/dist-packages (0.5.2)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.6/dist-packages (from konlpy) (1.18.5)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.6/dist-packages (from konlpy) (4.2.6)\n",
            "Requirement already satisfied: JPype1>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from konlpy) (0.7.5)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.6/dist-packages (from konlpy) (0.4.3)\n",
            "Requirement already satisfied: tweepy>=3.7.0 in /usr/local/lib/python3.6/dist-packages (from konlpy) (3.8.0)\n",
            "Requirement already satisfied: beautifulsoup4==4.6.0 in /usr/local/lib/python3.6/dist-packages (from konlpy) (4.6.0)\n",
            "Requirement already satisfied: PySocks>=1.5.7 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (1.7.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (1.12.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (1.3.0)\n",
            "Requirement already satisfied: requests>=2.11.1 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (2.23.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (3.1.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.11.1->tweepy>=3.7.0->konlpy) (2.9)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.11.1->tweepy>=3.7.0->konlpy) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.11.1->tweepy>=3.7.0->konlpy) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.11.1->tweepy>=3.7.0->konlpy) (2020.4.5.1)\n",
            "Done\n",
            "Installing mecab-0.996-ko-0.9.2.tar.gz.....\n",
            "Downloading mecab-0.996-ko-0.9.2.tar.gz.......\n",
            "from https://bitbucket.org/eunjeon/mecab-ko/downloads/mecab-0.996-ko-0.9.2.tar.gz\n",
            "--2020-06-14 02:01:08--  https://bitbucket.org/eunjeon/mecab-ko/downloads/mecab-0.996-ko-0.9.2.tar.gz\n",
            "Resolving bitbucket.org (bitbucket.org)... 18.205.93.1, 18.205.93.0, 18.205.93.2, ...\n",
            "Connecting to bitbucket.org (bitbucket.org)|18.205.93.1|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://bbuseruploads.s3.amazonaws.com/eunjeon/mecab-ko/downloads/mecab-0.996-ko-0.9.2.tar.gz?Signature=OwSPrGyap3SplNVMDBbIlFoI3eY%3D&Expires=1592101548&AWSAccessKeyId=AKIA6KOSE3BNJRRFUUX6&versionId=null&response-content-disposition=attachment%3B%20filename%3D%22mecab-0.996-ko-0.9.2.tar.gz%22 [following]\n",
            "--2020-06-14 02:01:08--  https://bbuseruploads.s3.amazonaws.com/eunjeon/mecab-ko/downloads/mecab-0.996-ko-0.9.2.tar.gz?Signature=OwSPrGyap3SplNVMDBbIlFoI3eY%3D&Expires=1592101548&AWSAccessKeyId=AKIA6KOSE3BNJRRFUUX6&versionId=null&response-content-disposition=attachment%3B%20filename%3D%22mecab-0.996-ko-0.9.2.tar.gz%22\n",
            "Resolving bbuseruploads.s3.amazonaws.com (bbuseruploads.s3.amazonaws.com)... 52.216.162.67\n",
            "Connecting to bbuseruploads.s3.amazonaws.com (bbuseruploads.s3.amazonaws.com)|52.216.162.67|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1414979 (1.3M) [application/x-tar]\n",
            "Saving to: ‘mecab-0.996-ko-0.9.2.tar.gz’\n",
            "\n",
            "mecab-0.996-ko-0.9. 100%[===================>]   1.35M  3.47MB/s    in 0.4s    \n",
            "\n",
            "2020-06-14 02:01:09 (3.47 MB/s) - ‘mecab-0.996-ko-0.9.2.tar.gz’ saved [1414979/1414979]\n",
            "\n",
            "Done\n",
            "Unpacking mecab-0.996-ko-0.9.2.tar.gz.......\n",
            "Done\n",
            "Change Directory to mecab-0.996-ko-0.9.2.......\n",
            "installing mecab-0.996-ko-0.9.2.tar.gz........\n",
            "configure\n",
            "make\n",
            "make check\n",
            "make install\n",
            "ldconfig\n",
            "Done\n",
            "Change Directory to /content\n",
            "Downloading mecab-ko-dic-2.1.1-20180720.tar.gz.......\n",
            "from https://bitbucket.org/eunjeon/mecab-ko-dic/downloads/mecab-ko-dic-2.1.1-20180720.tar.gz\n",
            "--2020-06-14 02:02:43--  https://bitbucket.org/eunjeon/mecab-ko-dic/downloads/mecab-ko-dic-2.1.1-20180720.tar.gz\n",
            "Resolving bitbucket.org (bitbucket.org)... 18.205.93.1, 18.205.93.0, 18.205.93.2, ...\n",
            "Connecting to bitbucket.org (bitbucket.org)|18.205.93.1|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://bbuseruploads.s3.amazonaws.com/a4fcd83e-34f1-454e-a6ac-c242c7d434d3/downloads/b5a0c703-7b64-45ed-a2d7-180e962710b6/mecab-ko-dic-2.1.1-20180720.tar.gz?Signature=H8yDyMGqIBAyWsotBjlWU0%2BebD8%3D&Expires=1592101622&AWSAccessKeyId=AKIA6KOSE3BNJRRFUUX6&versionId=tzyxc1TtnZU_zEuaaQDGN4F76hPDpyFq&response-content-disposition=attachment%3B%20filename%3D%22mecab-ko-dic-2.1.1-20180720.tar.gz%22 [following]\n",
            "--2020-06-14 02:02:43--  https://bbuseruploads.s3.amazonaws.com/a4fcd83e-34f1-454e-a6ac-c242c7d434d3/downloads/b5a0c703-7b64-45ed-a2d7-180e962710b6/mecab-ko-dic-2.1.1-20180720.tar.gz?Signature=H8yDyMGqIBAyWsotBjlWU0%2BebD8%3D&Expires=1592101622&AWSAccessKeyId=AKIA6KOSE3BNJRRFUUX6&versionId=tzyxc1TtnZU_zEuaaQDGN4F76hPDpyFq&response-content-disposition=attachment%3B%20filename%3D%22mecab-ko-dic-2.1.1-20180720.tar.gz%22\n",
            "Resolving bbuseruploads.s3.amazonaws.com (bbuseruploads.s3.amazonaws.com)... 52.216.106.43\n",
            "Connecting to bbuseruploads.s3.amazonaws.com (bbuseruploads.s3.amazonaws.com)|52.216.106.43|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 49775061 (47M) [application/x-tar]\n",
            "Saving to: ‘mecab-ko-dic-2.1.1-20180720.tar.gz’\n",
            "\n",
            "mecab-ko-dic-2.1.1- 100%[===================>]  47.47M  32.4MB/s    in 1.5s    \n",
            "\n",
            "2020-06-14 02:02:45 (32.4 MB/s) - ‘mecab-ko-dic-2.1.1-20180720.tar.gz’ saved [49775061/49775061]\n",
            "\n",
            "Done\n",
            "Unpacking  mecab-ko-dic-2.1.1-20180720.tar.gz.......\n",
            "Done\n",
            "Change Directory to mecab-ko-dic-2.1.1-20180720\n",
            "Done\n",
            "installing........\n",
            "configure\n",
            "make\n",
            "make install\n",
            "apt-get update\n",
            "apt-get upgrade\n",
            "apt install curl\n",
            "apt install git\n",
            "bash <(curl -s https://raw.githubusercontent.com/konlpy/konlpy/master/scripts/mecab.sh)\n",
            "Done\n",
            "Successfully Installed\n",
            "Now you can use Mecab\n",
            "from konlpy.tag import Mecab\n",
            "mecab = Mecab()\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p3pxdKtsDK4E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from konlpy.tag import *\n",
        "\n",
        "hannanum = Hannanum()\n",
        "kkma = Kkma()\n",
        "komoran = Komoran()\n",
        "okt = Okt()\n",
        "mecab = Mecab()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O3z0QlzhGEJl",
        "colab_type": "text"
      },
      "source": [
        "위 형태소 분석기들은 공통적으로 아래의 함수를 제공합니다.  \n",
        "nouns : 명사 추출  \n",
        "morphs : 형태소 추출  \n",
        "pos : 품사 부착"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YBTatyJgCcdB",
        "colab_type": "text"
      },
      "source": [
        "### 형태소 분석기 Okt"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_21rq8q93DgG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "4147256d-68de-47c6-9499-5c1d66e59386"
      },
      "source": [
        "print(okt.nouns(\"열심히 코딩한 당신, 연휴에는 여행을 가봐요\"))\n",
        "print(okt.morphs(\"열심히 코딩한 당신, 연휴에는 여행을 가봐요\"))\n",
        "print(okt.pos(\"열심히 코딩한 당신, 연휴에는 여행을 가봐요\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['코딩', '당신', '연휴', '여행']\n",
            "['열심히', '코딩', '한', '당신', ',', '연휴', '에는', '여행', '을', '가봐요']\n",
            "[('열심히', 'Adverb'), ('코딩', 'Noun'), ('한', 'Josa'), ('당신', 'Noun'), (',', 'Punctuation'), ('연휴', 'Noun'), ('에는', 'Josa'), ('여행', 'Noun'), ('을', 'Josa'), ('가봐요', 'Verb')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5HIC4M1nCnSQ",
        "colab_type": "text"
      },
      "source": [
        "### 형태소 분석기 꼬꼬마"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cK7W4wMy3Der",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "b3dcdcba-8ff5-4001-9e61-c85e39b3247b"
      },
      "source": [
        "print(kkma.nouns(\"열심히 코딩한 당신, 연휴에는 여행을 가봐요\"))\n",
        "print(kkma.morphs(\"열심히 코딩한 당신, 연휴에는 여행을 가봐요\"))\n",
        "print(kkma.pos(\"열심히 코딩한 당신, 연휴에는 여행을 가봐요\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['코딩', '당신', '연휴', '여행']\n",
            "['열심히', '코딩', '하', 'ㄴ', '당신', ',', '연휴', '에', '는', '여행', '을', '가보', '아요']\n",
            "[('열심히', 'MAG'), ('코딩', 'NNG'), ('하', 'XSV'), ('ㄴ', 'ETD'), ('당신', 'NP'), (',', 'SP'), ('연휴', 'NNG'), ('에', 'JKM'), ('는', 'JX'), ('여행', 'NNG'), ('을', 'JKO'), ('가보', 'VV'), ('아요', 'EFN')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dw5M9NoTDWxv",
        "colab_type": "text"
      },
      "source": [
        "### 형태소 분석기 코모란"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xYr7mO3C3DZc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "aa288ef1-2096-4c71-f33a-1841548989df"
      },
      "source": [
        "print(komoran.nouns(\"열심히 코딩한 당신, 연휴에는 여행을 가봐요\"))\n",
        "print(komoran.morphs(\"열심히 코딩한 당신, 연휴에는 여행을 가봐요\"))\n",
        "print(komoran.pos(\"열심히 코딩한 당신, 연휴에는 여행을 가봐요\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['코', '당신', '연휴', '여행']\n",
            "['열심히', '코', '딩', '하', 'ㄴ', '당신', ',', '연휴', '에', '는', '여행', '을', '가', '아', '보', '아요']\n",
            "[('열심히', 'MAG'), ('코', 'NNG'), ('딩', 'MAG'), ('하', 'XSV'), ('ㄴ', 'ETM'), ('당신', 'NNP'), (',', 'SP'), ('연휴', 'NNG'), ('에', 'JKB'), ('는', 'JX'), ('여행', 'NNG'), ('을', 'JKO'), ('가', 'VV'), ('아', 'EC'), ('보', 'VX'), ('아요', 'EC')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XSGyZZVkF_GS",
        "colab_type": "text"
      },
      "source": [
        "### 형태소 분석기 한나눔"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uvdqq2Lz3DUh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "4ac1e2d6-19c4-4189-fdb5-54e7ff1718e8"
      },
      "source": [
        "print(hannanum.nouns(\"열심히 코딩한 당신, 연휴에는 여행을 가봐요\"))\n",
        "print(hannanum.morphs(\"열심히 코딩한 당신, 연휴에는 여행을 가봐요\"))\n",
        "print(hannanum.pos(\"열심히 코딩한 당신, 연휴에는 여행을 가봐요\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['코딩', '당신', '연휴', '여행']\n",
            "['열심히', '코딩', '하', 'ㄴ', '당신', ',', '연휴', '에는', '여행', '을', '가', '아', '보', '아']\n",
            "[('열심히', 'M'), ('코딩', 'N'), ('하', 'X'), ('ㄴ', 'E'), ('당신', 'N'), (',', 'S'), ('연휴', 'N'), ('에는', 'J'), ('여행', 'N'), ('을', 'J'), ('가', 'P'), ('아', 'E'), ('보', 'P'), ('아', 'E')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w2XrKQ8ZGlVA",
        "colab_type": "text"
      },
      "source": [
        "### 형태소 분석기 Mecab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KhDAAPcmGzw7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "fbaad58a-a3c0-42d2-887a-d5a41ec632d8"
      },
      "source": [
        "print(mecab.nouns(\"열심히 코딩한 당신, 연휴에는 여행을 가봐요\"))\n",
        "print(mecab.morphs(\"열심히 코딩한 당신, 연휴에는 여행을 가봐요\"))\n",
        "print(mecab.pos(\"열심히 코딩한 당신, 연휴에는 여행을 가봐요\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['코딩', '당신', '연휴', '여행']\n",
            "['열심히', '코딩', '한', '당신', ',', '연휴', '에', '는', '여행', '을', '가', '봐요']\n",
            "[('열심히', 'MAG'), ('코딩', 'NNG'), ('한', 'XSA+ETM'), ('당신', 'NP'), (',', 'SC'), ('연휴', 'NNG'), ('에', 'JKB'), ('는', 'JX'), ('여행', 'NNG'), ('을', 'JKO'), ('가', 'VV'), ('봐요', 'EC+VX+EC')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JOXF6YzWKF1J",
        "colab_type": "text"
      },
      "source": [
        "각 형태소 분석기는 성능과 결과가 다르게 나오기 때문에, 형태소 분석기의 선택은 사용하고자 하는 필요 용도에 어떤 형태소 분석기가 가장 적절한지를 판단하고 사용하면 됩니다. 예를 들어서 속도를 중시한다면 메캅을 사용할 수 있습니다.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pquF1QYwKLcr",
        "colab_type": "text"
      },
      "source": [
        "## Sentence Tokenization(영어)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ic8zCrMlKRI3",
        "colab_type": "text"
      },
      "source": [
        "직관적으로 생각해봤을 때는 ?나 온점(.)이나 ! 기준으로 문장을 잘라내면 되지 않을까라고 생각할 수 있지만, 꼭 그렇지만은 않습니다. !나 ?는 문장의 구분을 위한 꽤 명확한 구분자(boundary) 역할을 하지만 온점은 꼭 그렇지 않기 때문입니다. 다시 말해, 온점은 문장의 끝이 아니더라도 등장할 수 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LcqfdjJpKUvp",
        "colab_type": "text"
      },
      "source": [
        "**IP 192.168.56.31 서버에 들어가서 로그 파일 저장해서 ukairia777@gmail.com로 결과 좀 보내줘. 그러고나서 점심 먹으러 가자.**  \n",
        "**Since I'm actively looking for Ph.D. students, I get the same question a dozen times every year.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "92Of0w2EKY0D",
        "colab_type": "text"
      },
      "source": [
        "온점을 기준으로 문장을 구분할 경우에는 예외사항이 너무 많습니다.  \n",
        "NLTK에서는 영어 문장의 토큰화를 수행하는 sent_tokenize를 지원하고 있습니다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a9AKFUkjKNXF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text = \"His barber kept his word. But keeping such a huge secret to himself was driving him crazy. Finally, the barber went up a mountain and almost to the edge of a cliff. He dug a hole in the midst of some reeds. He looked about, to mae sure no one was near.\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JJB-lEeNpJ38",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mpjJ4F_DKugH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "dd0bd6b9-a315-40d2-bf51-d4c071dd26e5"
      },
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "print(sent_tokenize(text))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['His barber kept his word.', 'But keeping such a huge secret to himself was driving him crazy.', 'Finally, the barber went up a mountain and almost to the edge of a cliff.', 'He dug a hole in the midst of some reeds.', 'He looked about, to mae sure no one was near.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "seKDLuUbKyn3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1845b620-14e7-4cf4-eb9c-6ddd8f40fdbe"
      },
      "source": [
        "text=\"I am actively looking for Ph.D. students. and you are a Ph.D student.\"\n",
        "print(sent_tokenize(text))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['I am actively looking for Ph.D. students.', 'and you are a Ph.D student.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w-BWco73K3x9",
        "colab_type": "text"
      },
      "source": [
        "## Sentence Tokenization(한국어)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SXDQxDBaK5nm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "outputId": "fe1998ae-e8ae-4888-f609-f77a523543a5"
      },
      "source": [
        "!pip install kss"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting kss\n",
            "  Downloading https://files.pythonhosted.org/packages/fc/bb/4772901b3b934ac204f32a0bd6fc0567871d8378f9bbc7dd5fd5e16c6ee7/kss-1.3.1.tar.gz\n",
            "Building wheels for collected packages: kss\n",
            "  Building wheel for kss (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for kss: filename=kss-1.3.1-cp36-cp36m-linux_x86_64.whl size=251594 sha256=07d4ca940ccaf35094fd4f65918e42e8ebbee5fdb84e250a425ffb77c920f23b\n",
            "  Stored in directory: /root/.cache/pip/wheels/8b/98/d1/53f75f89925cd95779824778725ee3fa36e7aa55ed26ad54a8\n",
            "Successfully built kss\n",
            "Installing collected packages: kss\n",
            "Successfully installed kss-1.3.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YwP2-MfNK9vD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "08745393-e782-4c70-b85b-77217d22e7ba"
      },
      "source": [
        "import kss\n",
        "text = '딥 러닝 자연어 처리가 재미있기는 합니다. 그런데 문제는 영어보다 한국어로 할 때 너무 어려워요. 농담아니에요. 이제 해보면 알걸요?'\n",
        "print(kss.split_sentences(text))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['딥 러닝 자연어 처리가 재미있기는 합니다.', '그런데 문제는 영어보다 한국어로 할 때 너무 어려워요.', '농담아니에요.', '이제 해보면 알걸요?']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DIdd-FB5Ln2r",
        "colab_type": "text"
      },
      "source": [
        "# Text Normalization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n5RMscXSLzWF",
        "colab_type": "text"
      },
      "source": [
        "텍스트 정규화(Text Normalization)은 통일할 수 있는 단어들은 하나로 통일하기 위한 전처리입니다. 보통 정규 표현식이나 자신만의 규칙을 정의할 수도 있지만 Stemming이나 Lemmatization도 사용합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TB3PvwnvLrsb",
        "colab_type": "text"
      },
      "source": [
        "## Stemming"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NpNtDgeMLtFr",
        "colab_type": "text"
      },
      "source": [
        "어간(Stem)을 추출하는 작업을 어간 추출(stemming)이라고 합니다. 어간 추출은 형태학적 분석을 단순화한 버전이라고 볼 수도 있고, 정해진 규칙만 보고 단어의 어미를 자르는 어림짐작의 작업이라고 볼 수도 있습니다. 다시 말해, 이 작업은 섬세한 작업이 아니기 때문에 어간 추출 후에 나오는 결과 단어는 사전에 존재하지 않는 단어일 수도 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3VpNxDOvMJH8",
        "colab_type": "text"
      },
      "source": [
        "가령, 포터 스테머의 포터 알고리즘의 어간 추출은 이러한 규칙들을 가집니다.  \n",
        "ALIZE → AL  \n",
        "ANCE → 제거  \n",
        "ICAL → IC  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xrz3HHV2MRHE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.stem import PorterStemmer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yVD5IJUMMZE9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "porterstemmer = PorterStemmer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M2xsnOIXMUIm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "2412254c-5ecb-45a1-9c00-4146fd48a935"
      },
      "source": [
        "text = \"This was not the map we found in Billy Bones's chest, but an accurate copy, complete in all things--names and heights and soundings--with the single exception of the red crosses and the written notes.\"\n",
        "words = word_tokenize(text)\n",
        "print(words)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['This', 'was', 'not', 'the', 'map', 'we', 'found', 'in', 'Billy', 'Bones', \"'s\", 'chest', ',', 'but', 'an', 'accurate', 'copy', ',', 'complete', 'in', 'all', 'things', '--', 'names', 'and', 'heights', 'and', 'soundings', '--', 'with', 'the', 'single', 'exception', 'of', 'the', 'red', 'crosses', 'and', 'the', 'written', 'notes', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lu5pnAyZMWTz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "55d46203-420b-4e94-f0e4-a8d9521cf393"
      },
      "source": [
        "print([porterstemmer.stem(w) for w in words])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['thi', 'wa', 'not', 'the', 'map', 'we', 'found', 'in', 'billi', 'bone', \"'s\", 'chest', ',', 'but', 'an', 'accur', 'copi', ',', 'complet', 'in', 'all', 'thing', '--', 'name', 'and', 'height', 'and', 'sound', '--', 'with', 'the', 'singl', 'except', 'of', 'the', 'red', 'cross', 'and', 'the', 'written', 'note', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "krn274GkMiV2",
        "colab_type": "text"
      },
      "source": [
        "조금 더 간단한 예제로 확인해봅시다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1VdyFjRYMmJH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "fa8b5c61-04aa-4eb0-9113-3bc091f5501e"
      },
      "source": [
        "words=['formalize', 'allowance', 'electricical']\n",
        "print([porterstemmer.stem(w) for w in words])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['formal', 'allow', 'electric']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AdhSbOSwT70z",
        "colab_type": "text"
      },
      "source": [
        "##Lemmatization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OUXhHIenUIP7",
        "colab_type": "text"
      },
      "source": [
        "표제어 추출은 단어들이 다른 형태를 가지더라도, 그 뿌리 단어를 찾아가서 단어의 개수를 줄일 수 있는지 판단합니다. 예를 들어서 am, are, is는 서로 다른 스펠링이지만 그 뿌리 단어는 be라고 볼 수 있습니다. 이 때, 이 단어들의 표제어는 be라고 합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7E4nsOpeUG4A",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "6a7319fc-feda-4acd-88d9-6bc9d07bd652"
      },
      "source": [
        "nltk.download('wordnet')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3oNo8EeZT9_q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "86348419-2cdf-4327-fffb-c3220f8adfd4"
      },
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "wordnetlemmatizer = WordNetLemmatizer()\n",
        "words=['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']\n",
        "print([wordnetlemmatizer.lemmatize(w) for w in words])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['policy', 'doing', 'organization', 'have', 'going', 'love', 'life', 'fly', 'dy', 'watched', 'ha', 'starting']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-9MGzYEfUcJT",
        "colab_type": "text"
      },
      "source": [
        "##Stemming and Normalization(한국어)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "htlAV5SDUd9w",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "outputId": "38615059-b6cf-41cf-d219-fa1a199eb0de"
      },
      "source": [
        "text = \"북한은 하루새 3차례에 걸쳐 대미·대남 압박 메시지를 내놓았다.\"\n",
        "print(okt.morphs(text))\n",
        "print(okt.morphs(text, stem=True))\n",
        "print(okt.pos(text))\n",
        "print(okt.pos(text, stem=True))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['북한', '은', '하루', '새', '3', '차례', '에', '걸쳐', '대미', '·', '대남', '압박', '메시지', '를', '내놓았다', '.']\n",
            "['북한', '은', '하루', '새', '3', '차례', '에', '걸치다', '대미', '·', '대남', '압박', '메시지', '를', '내놓다', '.']\n",
            "[('북한', 'Noun'), ('은', 'Josa'), ('하루', 'Noun'), ('새', 'Noun'), ('3', 'Number'), ('차례', 'Noun'), ('에', 'Josa'), ('걸쳐', 'Verb'), ('대미', 'Noun'), ('·', 'Punctuation'), ('대남', 'Noun'), ('압박', 'Noun'), ('메시지', 'Noun'), ('를', 'Josa'), ('내놓았다', 'Verb'), ('.', 'Punctuation')]\n",
            "[('북한', 'Noun'), ('은', 'Josa'), ('하루', 'Noun'), ('새', 'Noun'), ('3', 'Number'), ('차례', 'Noun'), ('에', 'Josa'), ('걸치다', 'Verb'), ('대미', 'Noun'), ('·', 'Punctuation'), ('대남', 'Noun'), ('압박', 'Noun'), ('메시지', 'Noun'), ('를', 'Josa'), ('내놓다', 'Verb'), ('.', 'Punctuation')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ac17Z6L9Vora",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "04aa671b-44f0-4e55-84f2-d73565b792b9"
      },
      "source": [
        "text = \"웃기는 소리하지마랔ㅋㅋㅋ\"\n",
        "print(okt.morphs(text))\n",
        "print(okt.morphs(text, norm=True))\n",
        "print(okt.pos(text))\n",
        "print(okt.pos(text, norm=True))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['웃기는', '소리', '하지마', '랔', 'ㅋㅋㅋ']\n",
            "['웃기는', '소리', '하지마라', 'ㅋㅋㅋ']\n",
            "[('웃기는', 'Verb'), ('소리', 'Noun'), ('하지마', 'Verb'), ('랔', 'Noun'), ('ㅋㅋㅋ', 'KoreanParticle')]\n",
            "[('웃기는', 'Verb'), ('소리', 'Noun'), ('하지마라', 'Verb'), ('ㅋㅋㅋ', 'KoreanParticle')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H-1tJ4cUWT5k",
        "colab_type": "text"
      },
      "source": [
        "#Stopwords"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9O5T28W_WXrq",
        "colab_type": "text"
      },
      "source": [
        "영어에서의 I, my, me, over, the와 같은 단어들이나 한국어의  조사, 접미사 같은 단어들은 문장에서는 자주 등장하지만 실제 의미 분석을 하는데는 거의 기여하는 바가 없는 경우가 있습니다. 이러한 단어들을 불용어(stopword)라고 하며, NLTK에서는 위와 같은 100여개 이상의 영어 단어들을 불용어로 패키지 내에서 미리 정의하고 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qSw-u1EjWoKA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "8d643614-5a8e-4502-b248-8601a9fd8b3b"
      },
      "source": [
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Kh0KJttWQ7E",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "4598f6a7-83f5-4bde-8cf5-c915a26dca9f"
      },
      "source": [
        "from nltk.corpus import stopwords  \n",
        "print(stopwords.words('english'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "heO9-yfpWyt7",
        "colab_type": "text"
      },
      "source": [
        "## Removing Stopwords"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PYDgcvEjW0XJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "16dd78fc-1c0d-429e-d9a0-d6e694c9bf94"
      },
      "source": [
        "example = \"Family is not an important thing. It's everything.\"\n",
        "stop_words = set(stopwords.words('english')) \n",
        "\n",
        "word_tokens = word_tokenize(example)\n",
        "\n",
        "result = []\n",
        "for w in word_tokens: \n",
        "    if w not in stop_words: \n",
        "        result.append(w) \n",
        "\n",
        "print('원문 :', word_tokens) \n",
        "print('불용어 제거 후 :', result) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "원문 : ['Family', 'is', 'not', 'an', 'important', 'thing', '.', 'It', \"'s\", 'everything', '.']\n",
            "불용어 제거 후 : ['Family', 'important', 'thing', '.', 'It', \"'s\", 'everything', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "huEic2cZXCDq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "43ccdec1-d159-4edf-f308-b70dcb548f70"
      },
      "source": [
        "example = \"고기를 아무렇게나 구우려고 하면 안 돼. 고기라고 다 같은 게 아니거든. 예컨대 삼겹살을 구울 때는 중요한 게 있지.\"\n",
        "stop_words = \"아무거나 아무렇게나 어찌하든지 같다 비슷하다 예컨대 이럴정도로 하면 아니거든\"\n",
        "\n",
        "# 위의 불용어는 명사가 아닌 단어 중에서 저자가 임의로 선정한 것으로 실제 의미있는 선정 기준이 아님\n",
        "stop_words = stop_words.split(' ')\n",
        "word_tokens = word_tokenize(example)\n",
        "\n",
        "result=[word for word in word_tokens if not word in stop_words]\n",
        "\n",
        "print('원문 :', word_tokens) \n",
        "print('불용어 제거 후 :', result) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "원문 : ['고기를', '아무렇게나', '구우려고', '하면', '안', '돼', '.', '고기라고', '다', '같은', '게', '아니거든', '.', '예컨대', '삼겹살을', '구울', '때는', '중요한', '게', '있지', '.']\n",
            "불용어 제거 후 : ['고기를', '구우려고', '안', '돼', '.', '고기라고', '다', '같은', '게', '.', '삼겹살을', '구울', '때는', '중요한', '게', '있지', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YSOnQ-SrXVVt",
        "colab_type": "text"
      },
      "source": [
        "# Integer Encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lMhAUtpFXjo5",
        "colab_type": "text"
      },
      "source": [
        "컴퓨터는 텍스트보다는 숫자를 더 잘 처리합니다. 이를 위해 자연어 처리에서는 텍스트를 숫자로 바꾸는 여러가지 기법들이 있습니다. 그리고 그러한 기법들을 본격적으로 적용시키기 위한 첫 단계로 각 단어를 고유한 정수에 맵핑(mapping)시키는 전처리 작업이 필요할 때가 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F-ZGnN3RXwhJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text = \"A barber is a person. a barber is good person. a barber is huge person. \\\n",
        "        he Knew A Secret! The Secret He Kept is huge secret. Huge secret. His barber kept his word. a barber kept his word. \\\n",
        "        His barber kept his secret. But keeping and keeping such a huge secret to himself was driving the barber crazy. \\\n",
        "        the barber went up a huge mountain.\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GOX6CnRoX7by",
        "colab_type": "text"
      },
      "source": [
        "## **Sentence Tokenizaiton**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3_XoZR2qX8PJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "84287b52-d411-431f-ebd2-cdb0f191036f"
      },
      "source": [
        "text = sent_tokenize(text)\n",
        "print(text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['A barber is a person.', 'a barber is good person.', 'a barber is huge person.', 'he Knew A Secret!', 'The Secret He Kept is huge secret.', 'Huge secret.', 'His barber kept his word.', 'a barber kept his word.', 'His barber kept his secret.', 'But keeping and keeping such a huge secret to himself was driving the barber crazy.', 'the barber went up a huge mountain.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jK59GuOqYkUT",
        "colab_type": "text"
      },
      "source": [
        "##**Word Tokenizaiton**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Ys8ZsTpajbe",
        "colab_type": "text"
      },
      "source": [
        "여기서 사용하는 전처리  \n",
        "1. 불용어 제거  \n",
        "2. 영어 단어의 소문자화  \n",
        "3. 길이가 짧은 단어 제거"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ma0UvwZVaYJn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "da82da75-8104-48e6-991c-472b7f819c11"
      },
      "source": [
        "text = sent_tokenize(text) # 문장 토큰화\n",
        "print(text)\n",
        "\n",
        "sentences = []\n",
        "stop_words = set(stopwords.words('english')) # NLTK 불용어\n",
        "\n",
        "for i in text:\n",
        "    sentence = word_tokenize(i) # 단어 토큰화\n",
        "    result = []\n",
        "\n",
        "    for word in sentence: \n",
        "        word = word.lower() # 모든 단어를 소문자화하여 단어의 개수를 줄입니다.\n",
        "        if word not in stop_words: # 불용어를 제거합니다.\n",
        "            if len(word) > 2: # 단어 길이가 2이하인 경우에 대하여 추가로 단어를 제거합니다.\n",
        "                result.append(word)\n",
        "    sentences.append(result) \n",
        "print(sentences)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['barber', 'person'], ['barber', 'good', 'person'], ['barber', 'huge', 'person'], ['knew', 'secret'], ['secret', 'kept', 'huge', 'secret'], ['huge', 'secret'], ['barber', 'kept', 'word'], ['barber', 'kept', 'word'], ['barber', 'kept', 'secret'], ['keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy'], ['barber', 'went', 'huge', 'mountain']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0BwtNf32a5zf",
        "colab_type": "text"
      },
      "source": [
        "##**Build Vocabulary(Python)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OCnSX_8Ba_iP",
        "colab_type": "text"
      },
      "source": [
        "단어 집합을 생성합니다. 단어 집합이란 중복을 제거한 단어들의 집합을 의미합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nPoOK_BGbDEV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import Counter"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "au-YB2RlbGM1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "d92af3a2-7f15-472c-db48-162cb83ff4b3"
      },
      "source": [
        "words = sum(sentences, [])\n",
        "print(words)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['barber', 'person', 'barber', 'good', 'person', 'barber', 'huge', 'person', 'knew', 'secret', 'secret', 'kept', 'huge', 'secret', 'huge', 'secret', 'barber', 'kept', 'word', 'barber', 'kept', 'word', 'barber', 'kept', 'secret', 'keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy', 'barber', 'went', 'huge', 'mountain']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bkw5xAbQbJLk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7b6ef226-125e-431a-8b7c-8248124ef926"
      },
      "source": [
        "vocab = Counter(words) # 파이썬의 Counter 모듈을 이용하면 단어의 모든 빈도를 쉽게 계산할 수 있습니다.\n",
        "print(vocab)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Counter({'barber': 8, 'secret': 6, 'huge': 5, 'kept': 4, 'person': 3, 'word': 2, 'keeping': 2, 'good': 1, 'knew': 1, 'driving': 1, 'crazy': 1, 'went': 1, 'mountain': 1})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xk7OBLT3bQQD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "63ca2206-7859-4a48-fd13-bd62071afe42"
      },
      "source": [
        "print(vocab[\"barber\"]) # 'barber'라는 단어의 빈도수 출력"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FRM6O58KcnXF",
        "colab_type": "text"
      },
      "source": [
        "##**Integer encoding(Python)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yf8cEDM4csbo",
        "colab_type": "text"
      },
      "source": [
        "빈도수가 높은 순서대로 정렬하고, 빈도수가 높을 수록 낮은 수의 정수를 부여합니다.  \n",
        "이렇게하면 빈도수가 낮은 단어들을 제외시키면서 단어 집합의 크기를 조절하기가 편합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nAa7PwgScG35",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "c8afda9e-f75c-4a6c-ca8e-eb109dde8999"
      },
      "source": [
        "# 빈도수가 높은 순서대로 정렬\n",
        "vocab_sorted = sorted(vocab.items(), key = lambda x:x[1], reverse = True)\n",
        "print(vocab_sorted)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('barber', 8), ('secret', 6), ('huge', 5), ('kept', 4), ('person', 3), ('word', 2), ('keeping', 2), ('good', 1), ('knew', 1), ('driving', 1), ('crazy', 1), ('went', 1), ('mountain', 1)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kSafwpl5cKvK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "aa3f6bb7-4712-471a-db6e-c72efbef89ef"
      },
      "source": [
        "# 이제 높은 빈도수를 가진 단어일수록 낮은 정수 인덱스를 부여합니다.\n",
        "word2idx = {}\n",
        "i=0\n",
        "for (word, frequency) in vocab_sorted :\n",
        "    if frequency > 1 : # 정제(Cleaning) 챕터에서 언급했듯이 빈도수가 적은 단어는 제외한다.\n",
        "        i = i+1\n",
        "        word2idx[word] = i\n",
        "print(word2idx)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5, 'word': 6, 'keeping': 7}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0-v75M6ec_FC",
        "colab_type": "text"
      },
      "source": [
        "자연어 처리를 하다보면, 텍스트 데이터에 있는 단어를 모두 사용하기 보다는 빈도수가 가장 높은 n개의 단어만 사용하고 싶은 경우가 많습니다. 위 단어들은 빈도수가 높은 순으로 낮은 정수가 부여되어져 있으므로 빈도수 상위 n개의 단어만 사용하고 싶다고하면 vocab에서 정수값이 1부터 n까지인 단어들만 사용하면 됩니다. 여기서는 상위 5개 단어만 사용한다고 가정하겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lZx6jOmCc_3M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab_size = 5\n",
        "words_frequency = [w for w,c in word2idx.items() if c >= vocab_size + 1] # 인덱스가 5 초과인 단어 제거\n",
        "for w in words_frequency:\n",
        "    del word2idx[w] # 해당 단어에 대한 인덱스 정보를 삭제"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "00jcJ0YMdF9n",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a65fdfb0-6dd3-40be-b3ac-3e470ff04cd7"
      },
      "source": [
        "print(word2idx)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BToOM2ebdVAy",
        "colab_type": "text"
      },
      "source": [
        "이제 word2idx에는 빈도수가 높은 상위 5개의 단어만 저장되었습니다. 이제 word2idx를 사용하여 단어 토큰화가 된 상태로 저장된 sentences에 있는 각 단어를 정수로 바꾸는 작업을 하겠습니다.\n",
        "\n",
        "예를 들어 sentences에서 첫번째 문장은 ['barber', 'person']이었는데, 이 문장에 대해서는 [1, 5]로 인코딩합니다. 그런데 두번째 문장인 ['barber', 'good', 'person']에는 더 이상 word2idx에는 존재하지 않는 단어인 'good'이라는 단어가 있습니다.  \n",
        "\n",
        "이처럼 단어 집합에 존재하지 않는 단어들을 Out-Of-Vocabulary(단어 집합에 없는 단어)의 약자로 'OOV'라고 합니다. word_to_index에 'OOV'란 단어를 새롭게 추가하고, 단어 집합에 없는 단어들은 'OOV'의 인덱스로 인코딩하겠습니다"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SMyrHedEeBs0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b23de7a4-fdfd-4d13-9061-767e1cc76eab"
      },
      "source": [
        "word2idx['OOV'] = len(word2idx) + 1\n",
        "print(word2idx)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5, 'OOV': 6}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y0B2e2aSePXq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        },
        "outputId": "e562c0ca-aa06-402d-8526-df096c058b20"
      },
      "source": [
        "sentences"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['barber', 'person'],\n",
              " ['barber', 'good', 'person'],\n",
              " ['barber', 'huge', 'person'],\n",
              " ['knew', 'secret'],\n",
              " ['secret', 'kept', 'huge', 'secret'],\n",
              " ['huge', 'secret'],\n",
              " ['barber', 'kept', 'word'],\n",
              " ['barber', 'kept', 'word'],\n",
              " ['barber', 'kept', 'secret'],\n",
              " ['keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy'],\n",
              " ['barber', 'went', 'huge', 'mountain']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EQ8E9JsWeIZx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        },
        "outputId": "37c53739-77df-40b6-a2ac-ae3d91833597"
      },
      "source": [
        "encoded = []\n",
        "for s in sentences:\n",
        "    temp = []\n",
        "    for w in s:\n",
        "        try:\n",
        "            temp.append(word2idx[w])\n",
        "        except KeyError:\n",
        "            temp.append(word2idx['OOV'])\n",
        "    encoded.append(temp)\n",
        "encoded"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[1, 5],\n",
              " [1, 6, 5],\n",
              " [1, 3, 5],\n",
              " [6, 2],\n",
              " [2, 4, 3, 2],\n",
              " [3, 2],\n",
              " [1, 4, 6],\n",
              " [1, 4, 6],\n",
              " [1, 4, 2],\n",
              " [6, 6, 3, 2, 6, 1, 6],\n",
              " [1, 6, 3, 6]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8T0spSnnfqMj",
        "colab_type": "text"
      },
      "source": [
        "## Build Vocab & Integer encoding(Tensorflow)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_c-U5DUfTkD",
        "colab_type": "text"
      },
      "source": [
        "방금 진행했던 Build Vocabulary와 Integer encoding을 텐서플로우가 제공하는 도구를 사용하면 훨씬 더 편하게 할 수도 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FmIHdSeRfdbO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "c1b7aded-1a39-438a-ca60-94a508af70c1"
      },
      "source": [
        "print(sentences)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['barber', 'person'], ['barber', 'good', 'person'], ['barber', 'huge', 'person'], ['knew', 'secret'], ['secret', 'kept', 'huge', 'secret'], ['huge', 'secret'], ['barber', 'kept', 'word'], ['barber', 'kept', 'word'], ['barber', 'kept', 'secret'], ['keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy'], ['barber', 'went', 'huge', 'mountain']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w_2KaNSUfg_w",
        "colab_type": "text"
      },
      "source": [
        "텐서플로우는 정수 인코딩을 수행하는 전처리 도구인 keras.preprocessing.text.Tokenizer를 제공합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2xE1esRlffJl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d6hGncjjiQmF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(sentences) # fit_on_texts()안에 코퍼스를 입력으로 하면 빈도수를 기준으로 단어 집합을 생성한다."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-u9AwjAoiXXx",
        "colab_type": "text"
      },
      "source": [
        "fit_on_texts는 입력한 텍스트로부터 단어 빈도수가 높은 순으로 낮은 정수 인덱스를 부여하는데, 정확히 앞서 설명한 정수 인코딩 작업이 이루어진다고 보면됩니다. 각 단어에 인덱스가 어떻게 부여되었는지를 보려면, word_index를 사용합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "isr6uBFIiTDc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "09991c60-8638-412d-9eca-12cce37d6adb"
      },
      "source": [
        "print(tokenizer.word_index)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5, 'word': 6, 'keeping': 7, 'good': 8, 'knew': 9, 'driving': 10, 'crazy': 11, 'went': 12, 'mountain': 13}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KqkI2svNiYW2",
        "colab_type": "text"
      },
      "source": [
        "각 단어의 빈도수가 높은 순서대로 인덱스가 부여된 것을 확인할 수 있습니다. 각 단어가 카운트를 수행하였을 때 몇 개였는지를 보고자 한다면 word_counts를 사용합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7JRzZfLmifsN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "644d2efe-c3e0-4721-b194-9e217ca34947"
      },
      "source": [
        "print(tokenizer.word_counts)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "OrderedDict([('barber', 8), ('person', 3), ('good', 1), ('huge', 5), ('knew', 1), ('secret', 6), ('kept', 4), ('word', 2), ('keeping', 2), ('driving', 1), ('crazy', 1), ('went', 1), ('mountain', 1)])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-4Tc7iLih83",
        "colab_type": "text"
      },
      "source": [
        "texts_to_sequences()는 입력으로 들어온 코퍼스에 대해서 각 단어를 이미 정해진 인덱스로 변환합니다.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "US-dKiN_iirh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ad78ff95-cec7-41f7-c9db-5f1899c5349f"
      },
      "source": [
        "print(tokenizer.texts_to_sequences(sentences))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1, 5], [1, 8, 5], [1, 3, 5], [9, 2], [2, 4, 3, 2], [3, 2], [1, 4, 6], [1, 4, 6], [1, 4, 2], [7, 7, 3, 2, 10, 1, 11], [1, 12, 3, 13]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k_7lqo1xindB",
        "colab_type": "text"
      },
      "source": [
        "앞서 빈도수가 가장 높은 단어 n개만을 사용하기 위해서 most_common()을 사용했었습니다. 케라스 토크나이저에서는 tokenizer = Tokenizer(num_words=숫자)와 같은 방법으로 빈도수가 높은 상위 몇 개의 단어만 사용하겠다고 지정할 수 있습니다. 여기서는 1번 단어부터 5번 단어까지만 사용하겠습니다. 상위 5개 단어를 사용한다고 토크나이저를 재정의 해보겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bshSoIRCioQ-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab_size = 5\n",
        "tokenizer = Tokenizer(num_words = vocab_size + 1) # 상위 5개 단어만 사용\n",
        "tokenizer.fit_on_texts(sentences)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "039BgD7wi0JR",
        "colab_type": "text"
      },
      "source": [
        "num_words에서 +1을 더해서 값을 넣어주는 이유는 num_words는 숫자를 0부터 카운트합니다. 만약 5를 넣으면 0 ~ 4번 단어 보존을 의미하게 되므로 뒤의 실습에서 1번 단어부터 4번 단어만 남게됩니다. 그렇기 때문에 1 ~ 5번 단어까지 사용하고 싶다면 num_words에 숫자 5를 넣어주는 것이 아니라 5+1인 값을 넣어주어야 합니다.\n",
        "\n",
        "실질적으로 숫자 0에 지정된 단어가 존재하지 않는데도 케라스 토크나이저가 숫자 0까지 단어 집합의 크기로 산정하는 이유는 자연어 처리에서 패딩(padding)이라는 작업 때문입니다. 이에 대해서는 뒤에 다루게 되므로 여기서는 케라스 토크나이저를 사용할 때는 숫자 0도 단어 집합의 크기로 고려해야한다고만 이해합시다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sahM_QG3irkc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4c21ba28-c265-4249-8d15-41d078a195c4"
      },
      "source": [
        "print(tokenizer.texts_to_sequences(sentences))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1, 5], [1, 5], [1, 3, 5], [2], [2, 4, 3, 2], [3, 2], [1, 4], [1, 4], [1, 4, 2], [3, 2, 1], [1, 3]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AybGCHFYivqy",
        "colab_type": "text"
      },
      "source": [
        "코퍼스에 대해서 각 단어를 이미 정해진 인덱스로 변환하는데, 상위 5개의 단어만을 사용하겠다고 지정하였으므로 1번 단어부터 5번 단어까지만 보존되고 나머지 단어들은 제거된 것을 볼 수 있습니다.  \n",
        "\n",
        "케라스 토크나이저는 기본적으로 단어 집합에 없는 단어인 OOV에 대해서는 단어를 정수로 바꾸는 과정에서 아예 단어를 제거한다는 특징이 있습니다. 단어 집합에 없는 단어들은 OOV로 간주하여 보존하고 싶다면 Tokenizer의 인자 oov_token을 사용합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s87WvIOKi7e6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ba5e9338-8453-47b6-f122-9b853172ef83"
      },
      "source": [
        "vocab_size = 5\n",
        "# 빈도수 상위 5개 단어만 사용. 숫자 0과 OOV를 고려해서 단어 집합의 크기는 +2\n",
        "tokenizer = Tokenizer(num_words = vocab_size + 2, oov_token = 'OOV')\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "print(tokenizer.texts_to_sequences(sentences))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[2, 6], [2, 1, 6], [2, 4, 6], [1, 3], [3, 5, 4, 3], [4, 3], [2, 5, 1], [2, 5, 1], [2, 5, 3], [1, 1, 4, 3, 1, 2, 1], [2, 1, 4, 1]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "croUFCyzjFVA",
        "colab_type": "text"
      },
      "source": [
        "만약 oov_token을 사용하기로 했다면 케라스 토크나이저는 기본적으로 'OOV'의 인덱스를 1로 합니다.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IAd2bfp9jGrK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6f6d8e3d-3d74-42c8-f3e0-1000a1df6676"
      },
      "source": [
        "print('단어 OOV의 인덱스 : {}'.format(tokenizer.word_index['OOV']))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "단어 OOV의 인덱스 : 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9N3kUBCIi-1R",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "48241f72-8a00-4cea-e917-007ea94a38d9"
      },
      "source": [
        "print(tokenizer.texts_to_sequences(sentences))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[2, 6], [2, 1, 6], [2, 4, 6], [1, 3], [3, 5, 4, 3], [4, 3], [2, 5, 1], [2, 5, 1], [2, 5, 3], [1, 1, 4, 3, 1, 2, 1], [2, 1, 4, 1]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BtS73b0-jIoK",
        "colab_type": "text"
      },
      "source": [
        "빈도수 상위 5개의 단어는 2 ~ 6까지의 인덱스를 가졌으며, 그 외 단어 집합에 없는 'good'과 같은 단어들은 전부 'OOV'의 인덱스인 1로 인코딩되었습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k9zsoaPWersA",
        "colab_type": "text"
      },
      "source": [
        "#Padding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Owuo7rMcew4l",
        "colab_type": "text"
      },
      "source": [
        "텍스트 데이터에 대해서 정수 인코딩을 수행하고나면, 이제 각 텍스트는 정수 시퀀스로 변환된 상태입니다. 정수 시퀀스로 변환된 각 문장(또는 문서)는 서로 길이가 다를 수 있습니다. 그런데 기계는 길이가 전부 동일한 문서들에 대해서는 하나의 행렬로 보고, 한꺼번에 묶어서 처리할 수 있습니다. 다시 말해 병렬 연산을 위해서 각 문서의 길이를 동일하게 맞춰주는 작업이 필요할 때가 있습니다. 실습을 통해 이해해봅시다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e3WAEwbakd7f",
        "colab_type": "text"
      },
      "source": [
        "##Numpy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uuEf9cQk3CSh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tCof-AJtv0Xn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentences = [['barber', 'person'], ['barber', 'good', 'person'], ['barber', 'huge', 'person'], \n",
        "             ['knew', 'secret'], ['secret', 'kept', 'huge', 'secret'], ['huge', 'secret'],\n",
        "             ['barber', 'kept', 'word'], ['barber', 'kept', 'word'], ['barber', 'kept', 'secret'],\n",
        "             ['keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy'],\n",
        "             ['barber', 'went', 'huge', 'mountain']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2uZHmAwWv1G3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(sentences) # fit_on_texts()안에 코퍼스를 입력으로 하면 빈도수를 기준으로 단어 집합을 생성한다."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BDNGWKsXv2L_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoded = tokenizer.texts_to_sequences(sentences)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jjUsd7AAv6I3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "188f15f3-690d-48b5-f2d4-54f09f138c47"
      },
      "source": [
        "print(encoded)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1, 5], [1, 8, 5], [1, 3, 5], [9, 2], [2, 4, 3, 2], [3, 2], [1, 4, 6], [1, 4, 6], [1, 4, 2], [7, 7, 3, 2, 10, 1, 11], [1, 12, 3, 13]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mdI4yCdFkLtI",
        "colab_type": "text"
      },
      "source": [
        "모두 동일한 길이로 맞춰주기 위해서 이 중에서 가장 길이가 긴 문장의 길이를 계산해보겠습니다.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iVDutHCQ0btI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "74d49a89-cfec-4521-a5a1-640d93f07c8f"
      },
      "source": [
        "max_len = max(len(item) for item in encoded)\n",
        "print(max_len)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BzpY2teNkNpr",
        "colab_type": "text"
      },
      "source": [
        "가장 길이가 긴 문장의 길이는 7입니다. 이제 모든 문장의 길이를 7로 맞춰주겠습니다. 이때 가상의 단어 'PAD'를 사용합니다. 'PAD'라는 단어가 있다고 가정하고, 이 단어는 0번 단어라고 정의해보겠습니다. 이제 길이가 7보다 짧은 문장에는 숫자 0을 채워서 전부 길이 7로 맞춰주겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1O6Z1wfns_um",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for item in encoded:                # for each item in the list\n",
        "    while len(item) < max_len:   # while the item length is smaller than 3\n",
        "        item.append(0)\n",
        "\n",
        "padded_np = np.array(encoded)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wne2VS1q0irJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        },
        "outputId": "2a91e70b-6495-4374-acd1-15beb9edddf5"
      },
      "source": [
        "padded_np"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 1,  5,  0,  0,  0,  0,  0],\n",
              "       [ 1,  8,  5,  0,  0,  0,  0],\n",
              "       [ 1,  3,  5,  0,  0,  0,  0],\n",
              "       [ 9,  2,  0,  0,  0,  0,  0],\n",
              "       [ 2,  4,  3,  2,  0,  0,  0],\n",
              "       [ 3,  2,  0,  0,  0,  0,  0],\n",
              "       [ 1,  4,  6,  0,  0,  0,  0],\n",
              "       [ 1,  4,  6,  0,  0,  0,  0],\n",
              "       [ 1,  4,  2,  0,  0,  0,  0],\n",
              "       [ 7,  7,  3,  2, 10,  1, 11],\n",
              "       [ 1, 12,  3, 13,  0,  0,  0]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vqo4ckHzkQjF",
        "colab_type": "text"
      },
      "source": [
        "길이가 7보다 짧은 문장에는 전부 숫자 0이 뒤로 붙어서 모든 문장의 길이가 전부 7이된 것을 알 수 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M4dlH7lCkk4X",
        "colab_type": "text"
      },
      "source": [
        "##Tensorflow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TI-auCxukmil",
        "colab_type": "text"
      },
      "source": [
        "실습을 이어서 하기 위해 패딩을 하기 전으로 초기화하겠습니다.  \n",
        "케라스에서는 위와 같은 패딩을 위한 도구 pad_sequences()를 제공하고 있습니다.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l6cv2hPY0v9S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoded = tokenizer.texts_to_sequences(sentences) # 패딩 초기화"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9U7bDBwdt15y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9a0f0e05-38fe-4316-9872-c839f8cc7796"
      },
      "source": [
        "print(encoded)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1, 5], [1, 8, 5], [1, 3, 5], [9, 2], [2, 4, 3, 2], [3, 2], [1, 4, 6], [1, 4, 6], [1, 4, 2], [7, 7, 3, 2, 10, 1, 11], [1, 12, 3, 13]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QTcWQMUCv9j_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "padded = pad_sequences(encoded)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O_LBXRwowIXg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        },
        "outputId": "d3766369-aff9-4693-b663-5f4b9c51d257"
      },
      "source": [
        "padded"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0,  0,  0,  0,  0,  1,  5],\n",
              "       [ 0,  0,  0,  0,  1,  8,  5],\n",
              "       [ 0,  0,  0,  0,  1,  3,  5],\n",
              "       [ 0,  0,  0,  0,  0,  9,  2],\n",
              "       [ 0,  0,  0,  2,  4,  3,  2],\n",
              "       [ 0,  0,  0,  0,  0,  3,  2],\n",
              "       [ 0,  0,  0,  0,  1,  4,  6],\n",
              "       [ 0,  0,  0,  0,  1,  4,  6],\n",
              "       [ 0,  0,  0,  0,  1,  4,  2],\n",
              "       [ 7,  7,  3,  2, 10,  1, 11],\n",
              "       [ 0,  0,  0,  1, 12,  3, 13]], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Azz5MZpeksxd",
        "colab_type": "text"
      },
      "source": [
        "Numpy로 패딩을 진행하였을 때와는 패딩 결과가 다른데 그 이유는 pad_sequences는 기본적으로 문서의 뒤에 0을 채우는 것이 아니라 앞에 0으로 채우기 때문입니다. 뒤에 0을 채우고 싶다면 인자로 padding='post'를 주면됩니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3BCgvyqmwKff",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "padded = pad_sequences(encoded, padding = 'post')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oRFOlBBqwTH3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        },
        "outputId": "c2303e27-2f4c-47b3-dc45-c8eb5ed4848f"
      },
      "source": [
        "padded "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 1,  5,  0,  0,  0,  0,  0],\n",
              "       [ 1,  8,  5,  0,  0,  0,  0],\n",
              "       [ 1,  3,  5,  0,  0,  0,  0],\n",
              "       [ 9,  2,  0,  0,  0,  0,  0],\n",
              "       [ 2,  4,  3,  2,  0,  0,  0],\n",
              "       [ 3,  2,  0,  0,  0,  0,  0],\n",
              "       [ 1,  4,  6,  0,  0,  0,  0],\n",
              "       [ 1,  4,  6,  0,  0,  0,  0],\n",
              "       [ 1,  4,  2,  0,  0,  0,  0],\n",
              "       [ 7,  7,  3,  2, 10,  1, 11],\n",
              "       [ 1, 12,  3, 13,  0,  0,  0]], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8c3I765Nkwlx",
        "colab_type": "text"
      },
      "source": [
        "Numpy를 이용하여 패딩을 했을 때와 결과가 동일합니다. 실제로 결과가 동일한지 두 결과를 비교합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CtAJAcXX0mBv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "80969d0b-c032-43a6-a371-9fc2c3814356"
      },
      "source": [
        "(padded==padded_np).all()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Nbn5i5eky_G",
        "colab_type": "text"
      },
      "source": [
        "True값이 리턴됩니다. 두 결과가 동일하다는 의미입니다. 지금까지는 가장 긴 길이를 가진 문서의 길이를 기준으로 패딩을 한다고 가정하였지만, 실제로는 꼭 가장 긴 문서의 길이를 기준으로 해야하는 것은 아닙니다. 가령, 모든 문서의 평균 길이가 20인데 문서 1개의 길이가 5,000이라고 해서 굳이 모든 문서의 길이를 5,000으로 패딩할 필요는 없을 수 있습니다. 이와 같은 경우에는 길이에 제한을 두고 패딩할 수 있습니다. max_len의 인자로 정수를 주면, 해당 정수로 모든 문서의 길이를 동일하게 합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fk8ntx1JxCN_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "padded = pad_sequences(encoded, padding = 'post', maxlen = 5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hO4IU_LTxEqv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        },
        "outputId": "d20eff35-ae6d-4a17-b2ce-a3da2efa9cb0"
      },
      "source": [
        "padded"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 1,  5,  0,  0,  0],\n",
              "       [ 1,  8,  5,  0,  0],\n",
              "       [ 1,  3,  5,  0,  0],\n",
              "       [ 9,  2,  0,  0,  0],\n",
              "       [ 2,  4,  3,  2,  0],\n",
              "       [ 3,  2,  0,  0,  0],\n",
              "       [ 1,  4,  6,  0,  0],\n",
              "       [ 1,  4,  6,  0,  0],\n",
              "       [ 1,  4,  2,  0,  0],\n",
              "       [ 3,  2, 10,  1, 11],\n",
              "       [ 1, 12,  3, 13,  0]], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aXBgfXtBk2e6",
        "colab_type": "text"
      },
      "source": [
        "길이가 5보다 짧은 문서들은 0으로 패딩되고, 기존에 5보다 길었다면 데이터가 손실됩니다. 숫자 0으로 패딩하는 것은 널리 퍼진 관례이긴 하지만, 반드시 지켜야하는 규칙은 아닙니다. 만약, 숫자 0이 아니라 다른 숫자를 패딩을 위한 숫자로 사용하고 싶다면 이 또한 가능합니다. 현재 사용된 정수들과 겹치지 않도록, 단어 집합의 크기에 +1을 한 숫자로 사용해봅시다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KIFIJbrGwi3O",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ba61a15c-2bd2-4f65-a699-e31615c40e8d"
      },
      "source": [
        "last_value = len(tokenizer.word_index) + 1\n",
        "print(last_value)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "14\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EJa3CsOjwUbH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "padded = pad_sequences(encoded, padding = 'post', value = last_value)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Svf8RL8AwfIJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        },
        "outputId": "b0734657-cb9e-4e45-ff69-5fdd6e75b295"
      },
      "source": [
        "padded"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 1,  5, 14, 14, 14, 14, 14],\n",
              "       [ 1,  8,  5, 14, 14, 14, 14],\n",
              "       [ 1,  3,  5, 14, 14, 14, 14],\n",
              "       [ 9,  2, 14, 14, 14, 14, 14],\n",
              "       [ 2,  4,  3,  2, 14, 14, 14],\n",
              "       [ 3,  2, 14, 14, 14, 14, 14],\n",
              "       [ 1,  4,  6, 14, 14, 14, 14],\n",
              "       [ 1,  4,  6, 14, 14, 14, 14],\n",
              "       [ 1,  4,  2, 14, 14, 14, 14],\n",
              "       [ 7,  7,  3,  2, 10,  1, 11],\n",
              "       [ 1, 12,  3, 13, 14, 14, 14]], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lMAiB3X_jWxx",
        "colab_type": "text"
      },
      "source": [
        "# One-Hot Encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LnqTpwLrlyrg",
        "colab_type": "text"
      },
      "source": [
        "단어 집합을 생성하고, 각 단어에 고유한 정수 인덱스를 부여하였다면 이제 이를 벡터로 변환할 수 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0H7qKHkbmTTv",
        "colab_type": "text"
      },
      "source": [
        "##Python"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TPVhIkKRjYJb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "25c2f17b-0aa1-4699-ef99-65cbc2dc377d"
      },
      "source": [
        "token = okt.morphs(\"나는 자연어 처리를 배운다\")  \n",
        "print(token)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['나', '는', '자연어', '처리', '를', '배운다']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JtvxYtFFl4A1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a18790d6-ee1f-460c-86dc-fe6932b5a1bb"
      },
      "source": [
        "word2idx={}\n",
        "for voca in token:\n",
        "     if voca not in word2idx.keys():\n",
        "       word2idx[voca] = len(word2idx)\n",
        "print(word2idx)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'나': 0, '는': 1, '자연어': 2, '처리': 3, '를': 4, '배운다': 5}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5tnik2lTmBvD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def one_hot_encoding(word, word2idx):\n",
        "       one_hot_vector = [0]*(len(word2idx))\n",
        "       index = word2idx[word]\n",
        "       one_hot_vector[index]=1\n",
        "       return one_hot_vector"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U4EkgMrDmF7q",
        "colab_type": "text"
      },
      "source": [
        "토큰을 입력하면 해당 토큰에 대한 원-핫 벡터를 만들어내는 함수를 만들었습니다.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G4RFledxmGzb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e0b50857-53a5-49d7-c89b-3f148afe3292"
      },
      "source": [
        "one_hot_encoding(\"자연어\", word2idx)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0, 0, 1, 0, 0, 0]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4no44Je7mXG_",
        "colab_type": "text"
      },
      "source": [
        "##Tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XlFuXOUvmgWX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.utils import to_categorical"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VcWgzL7WmZLe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text = \"나랑 점심 먹으러 갈래 점심 메뉴는 햄버거 갈래 갈래 햄버거 최고야\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hr_SnOihmar7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "59c43a3d-72c7-438b-c32c-2da7d7cd871d"
      },
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts([text])\n",
        "print(tokenizer.word_index) # 각 단어에 대한 인코딩 결과 출력."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'갈래': 1, '점심': 2, '햄버거': 3, '나랑': 4, '먹으러': 5, '메뉴는': 6, '최고야': 7}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0olnw_7xmnst",
        "colab_type": "text"
      },
      "source": [
        "위와 같이 생성된 단어 집합(vocabulary)에 있는 단어들로만 구성된 텍스트가 있다면, texts_to_sequences()를 통해서 이를 정수 시퀀스로 변환가능합니다. 생성된 단어 집합 내의 일부 단어들로만 구성된 서브 텍스트인 sub_text를 만들어 확인해보겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Bi9_RDRmiGz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d73a0069-1164-4321-dc4f-38d381d69132"
      },
      "source": [
        "sub_text = \"점심 먹으러 갈래 메뉴는 햄버거 최고야\"\n",
        "encoded = tokenizer.texts_to_sequences([sub_text])[0]\n",
        "print(encoded)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[2, 5, 1, 6, 3, 7]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l5FBDm3pmp5T",
        "colab_type": "text"
      },
      "source": [
        "지금까지 진행한 것은 이미 정수 인코딩 챕터에서 배운 내용입니다. 이제 해당 결과를 가지고, 원-핫 인코딩을 진행해보겠습니다. 케라스는 정수 인코딩 된 결과로부터 원-핫 인코딩을 수행하는 to_categorical()를 지원합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bnzb7N1Imko7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "outputId": "4179ccec-75cd-4c44-8625-d6cbee334ad4"
      },
      "source": [
        "one_hot = to_categorical(encoded)\n",
        "print(one_hot)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0. 0. 1. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 1. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 1.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NG4lQLiYmtB-",
        "colab_type": "text"
      },
      "source": [
        "위의 결과는 \"점심 먹으러 갈래 메뉴는 햄버거 최고야\"라는 문장이 [2, 5, 1, 6, 3, 7]로 정수 인코딩이 되고나서, 각각의 인코딩 된 결과를 인덱스로 원-핫 인코딩이 수행된 모습을 보여줍니다."
      ]
    }
  ]
}